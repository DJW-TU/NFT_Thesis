{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ee6695",
   "metadata": {},
   "source": [
    "### Document specifications\n",
    "Author: Dominik Wulf <br>\n",
    "Matriculation Number: 364 100 <br>\n",
    "Creation Date: 05. July 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27413b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import (TimeoutException,ElementNotSelectableException)\n",
    "from tqdm import tqdm\n",
    "\n",
    "import urllib.parse as parse\n",
    "import pandas as pd\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a03e86",
   "metadata": {},
   "source": [
    "### ! Insert File Path to Chromedriver here !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3550aa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dominik\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "# Insert file path of chromedriver here\n",
    "driver = webdriver.Chrome('Path to chromedriver')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec754bf6",
   "metadata": {},
   "source": [
    "### ! BEFORE START SET FILEPATH AND IMPORT DATA HERE ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e65eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data, please insert file path for '220704_project_website_links_nos.csv' here \n",
    "link_list = pd.read_csv(\"C:/Users/dominik/Documents/GitHub/Masterthesis/Scraper/Data/220704_project_website_links_nos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "256694ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://ens.domains/de/',\n",
       " 'https://boredapeyachtclub.com/#/',\n",
       " 'https://www.moonbirds.xyz/',\n",
       " 'https://otherside.xyz/',\n",
       " 'https://www.rtfkt.com/']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_list = link_list[\"links_no_utm\"].to_list()\n",
    "link_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b43e2316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define wait function\n",
    "ignore_list = [ElementNotSelectableException,TimeoutException]\n",
    "wait = WebDriverWait(driver, timeout=20, poll_frequency=1, ignored_exceptions=ignore_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d62e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_links = links of scraped page, link_list1 = link short list, that adds links of current page, link_list2 = link_long_list of all pages of current page, start_link = first link where scrape started\n",
    "def get_links(page_links,link_list1,link_list2,start_link):\n",
    "    # checks the number of links on the passed links of page\n",
    "    if len(page_links) <= 50:\n",
    "        # iterates over the passed page links\n",
    "        for i in page_links:\n",
    "            #checks if the link contains the initial starting page == equal to bage and not linkedin/youtube or smth else\n",
    "            if i.get_attribute(\"href\")[0:13] in start_link:\n",
    "                # adds link to list, if it is not present yet\n",
    "                link_list1.append(i.get_attribute(\"href\")) if i.get_attribute(\"href\") not in link_list2 else None\n",
    "            else:\n",
    "                # does nothing\n",
    "                pass\n",
    "    else:\n",
    "        print(\"More than 50 links. There are \" + str(len(page_links)) + \" links on the page\")\n",
    "    return link_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd89be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_links(start_link):\n",
    "    link_tree,base = get_link_tree(start_link)  \n",
    "    if len(link_tree)>0:\n",
    "        links = link_tree_iterate(link_tree,base)\n",
    "    else:\n",
    "        links = [None]\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44f8361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all links and save to a list - only append if not in list yet, start = initial link, from dappRadar\n",
    "def get_link_tree(start):\n",
    "    #creates empty list\n",
    "    links = []\n",
    "    base = None\n",
    "    try:\n",
    "        driver.get(start)\n",
    "        base = driver.current_url\n",
    "        time.sleep(1)\n",
    "        page_links = driver.find_elements(by=By.XPATH, value=\"//a[@href]\")\n",
    "        links = get_links(page_links,links,links,base)\n",
    "        links = list(filter(None, links))\n",
    "    except:\n",
    "        pass\n",
    "    return links, base if base is not None else links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18d49c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_tree_iterate(link_tree,start_link):\n",
    "    links2= []\n",
    "    for x in link_tree:\n",
    "        links2 = [] \n",
    "        try:\n",
    "            driver.get(x)\n",
    "            time.sleep(1)\n",
    "            page_links = driver.find_elements(by=By.XPATH, value=\"//a[@href]\")\n",
    "            links2 = get_links(page_links,links2,link_tree,start_link)\n",
    "            #after 1st iteratiom, link_tree needs to change to updated list of links\n",
    "            links2 = list(filter(None, links2))\n",
    "            link_tree = links2 + link_tree\n",
    "        except:\n",
    "            pass\n",
    "    return link_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f2de783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████                                                        | 1/3 [00:02<00:05,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More than 50 links. There are 545 links on the page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:11<00:00,  3.93s/it]\n"
     ]
    }
   ],
   "source": [
    "# get sublinks of NFT projects through iteration over link list\n",
    "link_tree_list = []\n",
    "for x in tqdm(link_list):\n",
    "    links = get_page_links(x)\n",
    "    links = list(set(links))\n",
    "    link_tree_list += links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44c143fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine starting links and merge starting and sublinks list together\n",
    "extracted_links = pd.Series(link_tree_list + link_list)\n",
    "extracted_links = extracted_links.to_frame()\n",
    "extracted_links.to_csv(\"C:/Users/dominik/Desktop/Uni/Masterthesis/Data/Outputs/Scrape/220906_sublinks_NFT_projects_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c01ac095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "extracted_links.columns = [\"websites\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70741e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove websites that are no part of NFT projects\n",
    "extracted_links = extracted_links[~extracted_links[\"websites\"].str.contains(\"linkedin\", na = False)]\n",
    "extracted_links = extracted_links[~extracted_links[\"websites\"].str.contains(\"cloudflare\", na = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "baa01af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all duplicates and NaN values from link list\n",
    "extracted_links = extracted_links.dropna()\n",
    "extracted_links = extracted_links.drop_duplicates(subset=[\"websites\"],keep = \"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8faa73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get number of websites in link list\n",
    "len(extracted_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b24bc896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>websites</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://boredapeyachtclub.com/#/mayc/terms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://boredapeyachtclub.com/#/home#roadmap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://boredapeyachtclub.com/#/terms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://boredapeyachtclub.com/#/mayc/home#buy-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://boredapeyachtclub.com/#/mayc/home#team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://boredapeyachtclub.com/#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://boredapeyachtclub.com/#/home#buy-an-ape</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://boredapeyachtclub.com/#/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://boredapeyachtclub.com/#/gallery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://boredapeyachtclub.com/#/home#team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://boredapeyachtclub.com/#/mayc/home#roadmap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://boredapeyachtclub.com/#/provenance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://www.moonbirds.xyz/terms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.moonbirds.xyz/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://ens.domains/de/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             websites\n",
       "1          https://boredapeyachtclub.com/#/mayc/terms\n",
       "2        https://boredapeyachtclub.com/#/home#roadmap\n",
       "3               https://boredapeyachtclub.com/#/terms\n",
       "4   https://boredapeyachtclub.com/#/mayc/home#buy-...\n",
       "5      https://boredapeyachtclub.com/#/mayc/home#team\n",
       "6                     https://boredapeyachtclub.com/#\n",
       "7     https://boredapeyachtclub.com/#/home#buy-an-ape\n",
       "8                    https://boredapeyachtclub.com/#/\n",
       "9             https://boredapeyachtclub.com/#/gallery\n",
       "10          https://boredapeyachtclub.com/#/home#team\n",
       "11  https://boredapeyachtclub.com/#/mayc/home#roadmap\n",
       "12         https://boredapeyachtclub.com/#/provenance\n",
       "13                    https://www.moonbirds.xyz/terms\n",
       "14                         https://www.moonbirds.xyz/\n",
       "15                            https://ens.domains/de/"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39d7653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe to list\n",
    "el_list = extracted_links[\"websites\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a163974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://boredapeyachtclub.com/#/mayc/terms',\n",
       " 'https://boredapeyachtclub.com/#/home#roadmap',\n",
       " 'https://boredapeyachtclub.com/#/terms',\n",
       " 'https://boredapeyachtclub.com/#/mayc/home#buy-an-ape',\n",
       " 'https://boredapeyachtclub.com/#/mayc/home#team']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc69c04",
   "metadata": {},
   "source": [
    "### Retrieve HTML content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7486a301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dominik\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "# insert location of chromedriver\n",
    "driver = webdriver.Chrome('C:/Users/dominik/Desktop/Python Projects/chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98fe948d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [01:03<00:00,  4.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# visit every website in link list and get the HTML content\n",
    "html_content = []\n",
    "links = []\n",
    "\n",
    "for x in tqdm(el_list):\n",
    "    try:\n",
    "        driver.get(x)\n",
    "        time.sleep(random.uniform(1.5,2.5))\n",
    "        page_text = driver.find_element(by=By.XPATH, value=\"/html/body\").text\n",
    "        html_content.append(page_text)\n",
    "        links.append(x)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5ba02c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save html content as dictionary to convert to dataframe\n",
    "d = {'links':links,'html_content':html_content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c7e7cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save html content as dataframe\n",
    "html_df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59d01d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save html content to csv\n",
    "html_df.to_csv(\"C:/Users/dominik/Desktop/Uni/Masterthesis/Data/Outputs/Scrape/220906_html_content.csv\",sep ='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
